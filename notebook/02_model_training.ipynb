{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2469a97b-d52e-44b8-9bf4-4f6ad6ff1c15",
   "metadata": {},
   "source": [
    "<img src=\"img/hpe_logo.png\" alt=\"HPE Logo\" width=\"125\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f7792-b70f-4899-9561-cb1486067d83",
   "metadata": {},
   "source": [
    "# HPE ML Platform Workshop - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085a37c-dfd7-4ed3-aff3-bb2e52ce0b1c",
   "metadata": {},
   "source": [
    "<img src='img/platform_step02_training.png' width='1200'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64af3fd7-cb62-41f9-98b4-8b98eb356960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet scikit-learn scikit-image pachyderm-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce4c5f9-decd-4aeb-a271-d934e55cea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from configparser import ConfigParser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Torch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Image modules\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import functions for downloading data\n",
    "from load_data import download_pach_repo, download_data, get_train_transforms\n",
    "\n",
    "# Import model util functions\n",
    "from model_utils import set_seed, plot_example, show_sample, plot_predictions, PairedRandomHorizontalFlip, PairedRandomAffine, PairedToTensor, DoubleConv,  InConv, Down, Up, OutConv, UNet\n",
    "\n",
    "# Import MLDE packages\n",
    "from determined.experimental import client as det\n",
    "from determined import pytorch\n",
    "\n",
    "# Import MLDM packages\n",
    "import pachyderm_sdk\n",
    "from pachyderm_sdk.api import pfs\n",
    "from pachyderm_sdk.api.pfs import File, FileType\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158f9c41-4f2e-41ef-899c-a6f5a7d6d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data and Set Variables\n",
    "config_obj = ConfigParser()\n",
    "config_obj.read(\"config.ini\")\n",
    "\n",
    "mldm_host = config_obj['PDK_INFO']['mldm_host']\n",
    "mldm_port = config_obj['PDK_INFO']['mldm_port']\n",
    "mlde_host = config_obj['PDK_INFO']['mlde_host']\n",
    "mlde_port = config_obj['PDK_INFO']['mlde_port']\n",
    "token = config_obj['PDK_INFO']['token']\n",
    "repo = config_obj['PDK_INFO']['repo']\n",
    "branch = config_obj['PDK_INFO']['branch']\n",
    "project = config_obj['PDK_INFO']['project']\n",
    "download_dir = config_obj['PDK_INFO']['download_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cc296-1c8e-4212-a1c7-38cbcd48c4bb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3228fee-a3b8-45e7-abd3-2d12b50aa1f7",
   "metadata": {},
   "source": [
    "<h2>Part 1: Processing, Loading and Analyzing Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe3369-110c-4067-9ffd-2981fc5ca601",
   "metadata": {},
   "source": [
    "<img src='img/platform_step01_data.png' width='1200'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a98820-f823-4d0f-92c8-d63340934a76",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0cea34-2709-4caf-9a60-1fcb62124d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Instance\n",
    "mldm_client = pachyderm_sdk.Client(mldm_host, mldm_port, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15bc7667-e3ca-41c3-b837-69dd63fbb4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception in callback <function MetadataClientInterceptor.intercept.<locals>.<lambda> at 0x7f30180aae50>: ConnectionError('Could not connect to pachyderm instance\\n')\n"
     ]
    },
    {
     "ename": "_MultiThreadedRendezvous",
     "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: INTERNAL: ipv4:34.71.146.200:80: Trying to connect an http1.x server\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: INTERNAL: ipv4:34.71.146.200:80: Trying to connect an http1.x server {grpc_status:14, created_time:\"2023-11-29T00:13:36.766971948+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m c_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m c_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_info \u001b[38;5;129;01min\u001b[39;00m mldm_client\u001b[38;5;241m.\u001b[39mpfs\u001b[38;5;241m.\u001b[39mwalk_file(file\u001b[38;5;241m=\u001b[39mFile\u001b[38;5;241m.\u001b[39mfrom_uri(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m      8\u001b[0m     f_path \u001b[38;5;241m=\u001b[39m file_info\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39mpath\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/run/determined/pythonuserbase/lib/python3.8/site-packages/pachyderm_sdk/api/pfs/__init__.py:1381\u001b[0m, in \u001b[0;36mApiStub.walk_file\u001b[0;34m(self, file, pagination_marker, number, reverse)\u001b[0m\n\u001b[1;32m   1378\u001b[0m request\u001b[38;5;241m.\u001b[39mnumber \u001b[38;5;241m=\u001b[39m number\n\u001b[1;32m   1379\u001b[0m request\u001b[38;5;241m.\u001b[39mreverse \u001b[38;5;241m=\u001b[39m reverse\n\u001b[0;32m-> 1381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__rpc_walk_file(request):\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[0;32m/run/determined/pythonuserbase/lib/python3.8/site-packages/grpc/_channel.py:426\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/determined/pythonuserbase/lib/python3.8/site-packages/grpc/_channel.py:826\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: INTERNAL: ipv4:34.71.146.200:80: Trying to connect an http1.x server\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: INTERNAL: ipv4:34.71.146.200:80: Trying to connect an http1.x server {grpc_status:14, created_time:\"2023-11-29T00:13:36.766971948+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "# List Files in the Repository\n",
    "files = []\n",
    "c_file = 0\n",
    "c_mask = 0\n",
    "c_folder = 0\n",
    "\n",
    "for file_info in mldm_client.pfs.walk_file(file=File.from_uri(f\"{project}/{repo}@{branch}\")):\n",
    "    f_path = file_info.file.path\n",
    "    print(f\"'{f_path}'\")\n",
    "    if \"_mask.tif\" in f_path:\n",
    "        c_mask += 1\n",
    "    elif \".tif\" in f_path:\n",
    "        c_file += 1\n",
    "    else:\n",
    "        c_folder += 1\n",
    "c_folder -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68f23d-2f02-49a7-87df-619b9bfec41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Files for local exploration\n",
    "# files = download_data(mldm_client, repo, branch, project, download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513eebd-9d7f-4595-bf3b-55c721370696",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = f\"{download_dir}/data1\"\n",
    "ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215bfce-6eac-4c62-be8a-bbc872fcb060",
   "metadata": {},
   "source": [
    "<h3> Data Exploration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2021068-9549-4f81-ba39-a39a7b43a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:   \n",
    "    # data preprocessing\n",
    "    data_dir = ROOT\n",
    "    logdir = 'logdir'\n",
    "    validation_fraction = 0.15\n",
    "    test_fraction = 0.10\n",
    "    train_batch = 16\n",
    "    valid_batch = 32\n",
    "    test_batch = 32\n",
    "    \n",
    "    # model setup\n",
    "    input_dim = 256\n",
    "    input_ch = 3\n",
    "    output_dim = 256\n",
    "    output_ch = 1\n",
    "    \n",
    "    # training\n",
    "    seed = 21\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b66fc8-0788-44ee-9054-2bd83a76677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd68d6-c380-498d-9050-9ed4724828e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs, images, masks = [], [], []\n",
    "for root, folders, files in os.walk(Config.data_dir):\n",
    "    for file in files:\n",
    "        # save only images with corresponding masks\n",
    "        if 'mask'in file:\n",
    "            dirs.append(root.replace(Config.data_dir, ''))\n",
    "            masks.append(file)\n",
    "            images.append(file.replace('_mask', ''))\n",
    "\n",
    "PathDF = pd.DataFrame({'directory': dirs, 'images': images, 'masks': masks})\n",
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9ca97-42e4-44d3-a948-eeae595cf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2rest = Config.validation_fraction + Config.test_fraction\n",
    "test2valid = Config.validation_fraction/train2rest\n",
    "\n",
    "train_df, rest = train_test_split(\n",
    "    PathDF, random_state=Config.seed,\n",
    "    test_size=train2rest\n",
    ")\n",
    "\n",
    "test_df, valid_df = train_test_split(\n",
    "    rest, random_state=Config.seed,\n",
    "    test_size=test2valid\n",
    ")\n",
    "\n",
    "print('Train:', train_df.shape[0])\n",
    "print('Valid:', valid_df.shape[0])\n",
    "print('Test:', test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0041d3-f442-4cc6-a39a-2bc5f24821d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(Config.data_dir, 2, test_df)\n",
    "plot_example(Config.data_dir, 3, test_df)\n",
    "plot_example(Config.data_dir, 14, test_df)\n",
    "plot_example(Config.data_dir, 16, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08518c3b-13ae-4318-851b-92a4ade24f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI_Dataset(Dataset):\n",
    "    def __init__(self, path_df, transform=None):\n",
    "        self.path_df = path_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.path_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_path = Config.data_dir + '/' +  self.path_df.iloc[idx]['directory']\n",
    "        img_path = os.path.join(base_path, self.path_df.iloc[idx]['images'])\n",
    "        mask_path = os.path.join(base_path, self.path_df.iloc[idx]['masks'])\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        sample = (image, mask)\n",
    "        # apply the same transform on both image and a mask\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada20fa7-10d0-43d7-bc35-cd2e5d0900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MRI_Dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabf86b-8461-4784-b92e-00bbf4237e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([                       \n",
    "    PairedRandomHorizontalFlip(), \n",
    "    PairedRandomAffine(\n",
    "        degrees=(-15, 15),\n",
    "        translate=(0.1, 0.1),\n",
    "        scale_ranges=(0.8, 1.2)\n",
    "    ),\n",
    "    PairedToTensor()\n",
    "])\n",
    "eval_transforms = PairedToTensor()\n",
    "\n",
    "train_data = MRI_Dataset(train_df, transform=train_transforms)\n",
    "valid_data = MRI_Dataset(valid_df, transform=eval_transforms)\n",
    "test_data = MRI_Dataset(test_df, transform=eval_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=Config.train_batch, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_data, batch_size=Config.valid_batch, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=Config.test_batch, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276369b-4021-4c8c-8c10-4dcbb42eb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, device=Config.device):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc='Iterating over train data')\n",
    "    for imgs, masks in pbar:\n",
    "        # pass to device\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # forward\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, masks)\n",
    "        running_loss += loss.item()*imgs.shape[0] \n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    running_loss /= len(train_loader.sampler)\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6333a80-49ed-493d-a2dc-666e68475de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, criterion, eval_loader, device=Config.device):\n",
    "    running_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy, f1_scores = [], []\n",
    "        pbar = tqdm(eval_loader, desc='Iterating over evaluation data')\n",
    "        for imgs, masks in pbar:\n",
    "            # pass to device\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            # forward\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, masks)\n",
    "            running_loss += loss.item()*imgs.shape[0]\n",
    "            # calculate predictions using output\n",
    "            predicted = (out > 0.5).float()\n",
    "            predicted = predicted.view(-1).cpu().numpy()\n",
    "            labels = masks.view(-1).cpu().numpy()\n",
    "            accuracy.append(accuracy_score(labels, predicted))\n",
    "            f1_scores.append(f1_score(labels, predicted))\n",
    "    acc = sum(accuracy)/len(accuracy)\n",
    "    f1 = sum(f1_scores)/len(f1_scores)\n",
    "    running_loss /= len(eval_loader.sampler)\n",
    "    return {\n",
    "        'accuracy':acc,\n",
    "        'f1_macro':f1, \n",
    "        'loss':running_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbab58-c1e8-46a3-9f4e-4291430125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader,\n",
    "          device=Config.device, \n",
    "          num_epochs=Config.epochs, \n",
    "          valid_loss_min=np.inf):\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        # train for epoch\n",
    "        train_loss = train_loop(\n",
    "            model, optimizer, criterion, train_loader, device=device)\n",
    "        # evaluate on validation set\n",
    "        metrics = eval_loop(\n",
    "            model, criterion, valid_loader, device=device\n",
    "        )\n",
    "        # show progress\n",
    "        print_string = f'Epoch: {e+1} '\n",
    "        print_string+= f'TrainLoss: {train_loss:.5f} '\n",
    "        print_string+= f'ValidLoss: {metrics[\"loss\"]:.5f} '\n",
    "        print_string+= f'ACC: {metrics[\"accuracy\"]:.5f} '\n",
    "        print_string+= f'F1: {metrics[\"f1_macro\"]:.3f}'\n",
    "        print(print_string)\n",
    "\n",
    "        # save the model \n",
    "        if metrics[\"loss\"] <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), 'UNet.pt')\n",
    "            valid_loss_min = metrics[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d8200-3a9a-442f-8cfe-8dd250a4d102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(Config.seed)\n",
    "model = UNet(Config.input_ch, Config.output_ch).to(Config.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "train(model, optimizer, criterion, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16304133-f5a0-4ba8-8c72-be219afea6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca0cef-3262-4c49-830e-9cf59cd6fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(Config.data_dir, model, 2, Config.device, test_df, eval_transforms)\n",
    "plot_predictions(Config.data_dir, model, 3, Config.device, test_df, eval_transforms)\n",
    "plot_predictions(Config.data_dir, model, 14, Config.device, test_df, eval_transforms)\n",
    "plot_predictions(Config.data_dir, model, 16, Config.device, test_df, eval_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d0d05-145d-48c2-9e09-856f6b0235f6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e813b2-6035-4d02-a296-6607d6afab06",
   "metadata": {},
   "source": [
    "<h2> Part 2: Training at Scale </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a98fa-b848-4443-bd67-f479faaa2ac8",
   "metadata": {},
   "source": [
    "<img src='img/platform_step02_training.png' width='1200'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1ff80-3809-4ab6-a03c-999115bff2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is a hosted environment, the connection is done automatically through environment variables\n",
    "# mlde_url = f\"{mlde_host}:{mlde_port}\"\n",
    "# det.login(mlde_url, \"admin\", mlde_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9b3c0-7b15-449e-818c-b1f136bbc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment using yaml file\n",
    "exp = det.create_experiment(config=\"./experiment/const.yaml\", model_dir=\"./experiment/\")\n",
    "print(f\"started experiment {exp.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31906254-8a25-4fe8-9122-5dc6b1b93ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for experiment to complete and print exit status\n",
    "exit_status = exp.wait()\n",
    "print(f\"experiment completed with status {exit_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9054234-d719-483f-a0fb-8a6a92487b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c3541-0a8a-4344-ad33-061768ebfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best Checkpoint of the experiment and print uuid\n",
    "best_checkpoint = exp.top_checkpoint()\n",
    "best_checkpoint_uuid = best_checkpoint.uuid\n",
    "print(f\"Best checkpoint was {best_checkpoint_uuid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f978cd5-be20-4f5c-ad8f-65c456371e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26746a0-a867-44c3-9c34-8a54dad050d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = det.get_experiment(experiment_id).top_checkpoint()\n",
    "path = checkpoint.download()\n",
    "mlde_model = pytorch.load_trial_from_checkpoint_path(path).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8b5fa-ae31-402d-a800-5debb325c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d80aab-5c2b-4254-958e-d1242c1b1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0ed0d-a7b5-410b-aebc-08190161e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_new_predictions(data_dir, model, idx, device, test_df, eval_transforms):\n",
    "    base_path = data_dir + '/' +  test_df['directory'].iloc[idx]\n",
    "    img_path = os.path.join(base_path, test_df['images'].iloc[idx])\n",
    "    mask_path = os.path.join(base_path, test_df['masks'].iloc[idx])\n",
    "    \n",
    "    size = 256\n",
    "    shape = [1, 256, 256]\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    transforms = [torchvision.transforms.ToTensor()]\n",
    "    try:\n",
    "        width, height = size\n",
    "    except TypeError:\n",
    "        width = height = size\n",
    "    scale = min(width / img.width, height / img.height)\n",
    "    new_width, new_height = int(img.width * scale), int(img.height * scale)\n",
    "    diff_width, diff_height = width - new_width, height - new_height\n",
    "    resize = torchvision.transforms.Resize(size=(new_height, new_width))\n",
    "    pad = torchvision.transforms.Pad(\n",
    "        padding=(\n",
    "            diff_width // 2,\n",
    "            diff_height // 2,\n",
    "            diff_width // 2 + diff_width % 2,\n",
    "            diff_height // 2 + diff_height % 2,\n",
    "        )\n",
    "    )\n",
    "    transforms = [resize, pad] + transforms\n",
    "    transformation = torchvision.transforms.Compose(transforms)\n",
    "    x = transformation(img)\n",
    "    x = torch.stack([x], 0)\n",
    "    \n",
    "    mask = Image.open(mask_path)\n",
    "\n",
    "    preds = model(x)\n",
    "  \n",
    "    pred_values = torch.tensor(preds[0], requires_grad=True)\n",
    "    pred_values = pred_values.detach().numpy()\n",
    "    pred_output = torch.Tensor(np.array(pred_values).reshape(shape))    \n",
    "\n",
    "    plot_images = {'Image': img, \n",
    "                   'Mask': mask, \n",
    "                   'Predicted Mask': pred_output.permute(1, 2, 0)}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(16,4))\n",
    "    for i, key in enumerate(plot_images.keys()):\n",
    "        ax[i].imshow(plot_images[key])\n",
    "        ax[i].set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613f380-b8d4-4317-8aac-962d727f06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_new_predictions(Config.data_dir, mlde_model, 2, Config.device, test_df, eval_transforms)\n",
    "plot_new_predictions(Config.data_dir, mlde_model, 3, Config.device, test_df, eval_transforms)\n",
    "plot_new_predictions(Config.data_dir, mlde_model, 14, Config.device, test_df, eval_transforms)\n",
    "plot_new_predictions(Config.data_dir, mlde_model, 16, Config.device, test_df, eval_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58ac6f-affe-4187-9fe6-7ac147e15640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f8ed1b-feda-412d-a6e2-9b85ad4159c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed, Hyperparameter Search Training Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5a1c2-d084-4a5e-b6ae-162f395f79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment using yaml file\n",
    "# exp = det.create_experiment(config=\"./experiment/search.yaml\", model_dir=\"./experiment/\")\n",
    "# print(f\"started experiment {exp.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350aa1c-fc53-4202-a4cb-df2b31bf261a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef5193-8cf1-46ce-bd9a-27186ca0c653",
   "metadata": {},
   "source": [
    "<h2> Part 3: Deploying Models to Production </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a118db0-2f21-408f-b446-5a8e85a3f062",
   "metadata": {},
   "source": [
    "<img src='img/platform_step03_deployment.png' width='1200'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b0c2b-61d3-4fa9-93ee-b775d0bcaa35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15895416-c01c-4fc4-9e70-92577bf9dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving Predictions from the Production Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a268136-5bfb-4feb-95f4-d461caf2ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978861c-433d-4be1-ad45-721cfb077fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config_obj['PDK_INFO']['model_name']\n",
    "ingress_host = config_obj['PDK_INFO']['ingress_host']\n",
    "ingress_port = config_obj['PDK_INFO']['ingress_port']\n",
    "service_hostname = config_obj['PDK_INFO']['service_hostname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf28e1-fb8f-468f-aa5d-46e85151f244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f3dd6-c119-4ff4-be2c-0c6347a99012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to caluclate intersection over union of prediction\n",
    "def iou(pred, label):\n",
    "    intersection = (pred * label).sum()\n",
    "    union = pred.sum() + label.sum() - intersection\n",
    "    if pred.sum() == 0 and label.sum() == 0:\n",
    "        return 1\n",
    "    return intersection / union\n",
    "\n",
    "# Function to create tensor for image and mask\n",
    "def PairedToTensor(sample):\n",
    "    img, mask = sample\n",
    "    img = np.array(img)\n",
    "    mask = np.expand_dims(mask, -1)\n",
    "    img = np.moveaxis(img, -1, 0)\n",
    "    mask = np.moveaxis(mask, -1, 0)\n",
    "    img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
    "    img = img/255\n",
    "    mask = mask/255\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f709c2e-80a1-4758-aa8b-42412d87672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and mask\n",
    "image = Image.open(\"data/brain_mri/data1/TCGA_CS_6290_20000917/TCGA_CS_6290_20000917_10.tif\")\n",
    "mask = Image.open(\"data/brain_mri/data1/TCGA_CS_6290_20000917/TCGA_CS_6290_20000917_10_mask.tif\")\n",
    "\n",
    "# Create tuple\n",
    "sample = (image, mask)\n",
    "\n",
    "# Create tensors from tuple\n",
    "tensor_sample = PairedToTensor(sample)\n",
    "\n",
    "# Create JSON payload for request\n",
    "data = np.array(tensor_sample[0])\n",
    "data_shape = list(data.shape)\n",
    "request = {\n",
    "    \"inputs\": [{\n",
    "        \"name\": str(uuid.uuid4()),\n",
    "        \"shape\": data_shape,\n",
    "        \"datatype\": \"FP32\",\n",
    "        \"data\": np.round(data, 4).tolist()\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Show image that will be submitted\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title(f'Submitted Image: ')\n",
    "plt.imshow(tensor_sample[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df1f80-3eb1-4262-a031-99acd589be87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e871c26-748a-44d1-bf28-d4f47c0f2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPC request for Prediction (header, URL, payload)\n",
    "url = str(\"http://\") + str(ingress_host) + \":\" + str(ingress_port) + \"/v1/models/\" + str(model_name) + \":predict\"\n",
    "headers = {'Host': service_hostname}\n",
    "payload = json.dumps(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4f2f8-8e30-42b9-93eb-09845b6b025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit request, extract prediction in JSON, transform to Tensor\n",
    "response = requests.post(url, data=payload, headers=headers)\n",
    "output = response.json()\n",
    "\n",
    "shape = output[\"outputs\"][0][\"shape\"]\n",
    "values = output[\"outputs\"][0][\"data\"]\n",
    "output = torch.Tensor(np.array(values).reshape(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afacf51-e437-4b2b-94f8-bb810cf432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display groundtruth and prediction mask, call iou function and display iou\n",
    "f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
    "axarr[0].imshow(tensor_sample[1].permute(1, 2, 0), alpha=0.8)\n",
    "axarr[0].title.set_text(f'Mask (Ground Truth):')\n",
    "axarr[1].imshow(output.permute(1, 2, 0), alpha=0.8)\n",
    "axarr[1].title.set_text(f'Mask (Prediction):')\n",
    "print(f'Intersection over Union (IoU): {iou(output, tensor_sample[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63447b06-24bc-4c02-8df9-bb32d473ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display groundtruth and prediction overlaid on submitted image, call iou function and display iou\n",
    "f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
    "axarr[0].imshow(tensor_sample[0].permute(1, 2, 0))\n",
    "axarr[0].imshow(tensor_sample[1].permute(1, 2, 0), alpha=0.4)\n",
    "axarr[0].title.set_text(f'Full Image (Ground Truth):')\n",
    "axarr[1].imshow(tensor_sample[0].permute(1, 2, 0))\n",
    "axarr[1].imshow(output.permute(1, 2, 0), alpha=0.4)\n",
    "axarr[1].title.set_text(f'Full Image (Prediction):')\n",
    "print(f'Intersection over Union (IoU): {iou(output, tensor_sample[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500af3a-e14e-4293-85c8-453d567ae63a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a931302a-c0c7-4880-bdad-b2a02d7174ab",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d1965-34be-4a8f-ad30-c3e203403ba3",
   "metadata": {},
   "source": [
    "<h2> Bringing It All Together </h2>\n",
    "<img src='img/big_picture.png' width='1200'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ecdfa-bad5-438f-a636-972456d5ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45363fa6-3ed1-42e9-bb2c-0ae50e196ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = './data/brain_mri/data1/'\n",
    "name = f\"/data{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dfeb6-4a6c-4a72-b864-dff77342f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(source_data_dir):\n",
    "    commit_branch = pfs.Branch.from_uri(f\"{project}/{repo}@{branch}\")\n",
    "    with mldm_client.pfs.commit(branch=commit_branch) as commit:\n",
    "        commit.put_files(source=source_data_dir, path=name)\n",
    "    return commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992aea4-15e6-4c9c-ab5c-ec8493579cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the commits to finish\n",
    "print(\"Waiting for commits to finish...\")\n",
    "d_commit = insert_data(source_dir)\n",
    "print(d_commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b260f-1162-41d6-9ea5-1c51e2a9946d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
